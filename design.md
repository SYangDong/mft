#abstract

目前深度神经网络领域在飞速发展，研究员们研究出许多高性能的深度模型，然而很多模型在推理过程中因为推理时间的问题不能够满足实际部署要求。为了缓解这个问题，我们提出一种MCMC算法，找出最佳的模型并行、数据并行以及层融合的配置，从而使加速卡片上的资源利用率最高来加快推理过程。与此同时，我们将该种算法运用在GPU和MLU上，实验显示我们MFT的性能超过了CUDNN（或者框架）以及CNML的性能。最后在输入数据可变的情况下，我们通过算法搜索，保存不同的配置，其实验结果证明它的推理性能较框架也有较大提升。

#introduction

目前深度学习领域正在高速发展，研究员们提出了各种高性能的模型，然而在实际部署中，很多模型不能够满足实际运用的要求。 为了缓解这种问题: [1]提出了模型并行和数据并行。[2]提出了模型并行、数据并行以及参数并行。 [3]提出了剪枝的方法， [4]提出了量化的方法，这些方法均实现了推理性能上的提升，而且这些方法都够一起使用，提升性能的效果对增强。

本文提出一种MCMC的算法，通过搜索获取获取最优的模型并行、数据并行以及层融合的配置，并将它运用在推理过程中。在动态输入的模型中，我们通过搜索会保存不同的算子配置，运行时我们可以根据实际的输入大小选择其所在区间的配置，然后运行其配置对应的核函数。

本文的结构如下：
第二节：介绍相关的内容，模型并行，数据并行，层融合的概念。第三节提出我们的方法，以及他的具体实现，第四节：实验的对比，证明我们的算法的有效性，第五节总结

#Our motheod
##problem definition

1. 搜索空间的定义
    总时间 = 数据搬运的时间 + 计算时间

2. 搜索的方法
    怎么建立模型

3. 动态输入的处理
    采用桶的概念，怎么设置区间

#Experiment

1. 我们算法的高效行，算法搜索时间较短

2. 模型性能上的提升： NMT，resnet50等等

3. 动态输入的性能的提升

4. 我们的计算的理论时间和实际时间的对比，证明我们算法的有效性

#conclusion
